[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "3d-Printed Microsopes",
    "section": "",
    "text": "This site contains resources related to learning about computational microscopy by building your very own 3d-printed microscope based around a Raspberry Pi single-board computer and a Raspberry Pi camera module.\nYou can find information about getting started on the Labs page.\n\n\n\n\n\n\nFigure 1: A photo of the 3d-printed microscopy platform."
  },
  {
    "objectID": "labs/lab2/index.html",
    "href": "labs/lab2/index.html",
    "title": "Introduction to Ray Tracing",
    "section": "",
    "text": "By the end of this lab you will have…\n\nSet up and understand how to configure optical ray tracing simulations using the Python package RayTracing.\nPerform ray tracing simulations of simple imaging systems and analyze their performance"
  },
  {
    "objectID": "labs/lab2/index.html#learning-objectives",
    "href": "labs/lab2/index.html#learning-objectives",
    "title": "Introduction to Ray Tracing",
    "section": "",
    "text": "By the end of this lab you will have…\n\nSet up and understand how to configure optical ray tracing simulations using the Python package RayTracing.\nPerform ray tracing simulations of simple imaging systems and analyze their performance"
  },
  {
    "objectID": "labs/lab2/index.html#setup",
    "href": "labs/lab2/index.html#setup",
    "title": "Introduction to Ray Tracing",
    "section": "2 Setup",
    "text": "2 Setup\n\n2.1 Download and Install the RayTracing Package\nhttps://github.com/DCC-Lab/RayTracing\nhttps://raytracing.readthedocs.io/\nThe first step in this lab is to set up the RayTracing Python package. You may do this using either Spyder on your personal machine or by using a Jupyter notebook either locally or in Google Colaboratory (Colab). In this lab we will use an Jupyter notebook hosted on Google Colab.\nTo start, open the Colab notebook here and save a copy to your Google Drive. The Jupyter Notebook walks you through the steps necessary to set up and use the RayTracing package to perform a simple simulation. Follow the steps to try it out for yourself and get familiar with the basic interface. You may also find the catalog of examples listed here (accompanying code for easy copy-paste in the GitHub repository here)"
  },
  {
    "objectID": "labs/lab2/index.html#analysis-of-finite-and-infinite-conjugate-optical-systems",
    "href": "labs/lab2/index.html#analysis-of-finite-and-infinite-conjugate-optical-systems",
    "title": "Introduction to Ray Tracing",
    "section": "3 Analysis of Finite and Infinite Conjugate Optical Systems",
    "text": "3 Analysis of Finite and Infinite Conjugate Optical Systems\n\n3.0.1 Ray Tracing of a Finite Conjugate Optical System\nIn this section you will perform a simulation of a finite conjugate microscope. This system is composed of two lenses: the microscope objective and the tube lens. The objective forms an intermediate image which is then used by the tube lens to form the final image. This system is called finite conjugate because the intermediate image plane is at a finite distance. This is in contrast to the infinite conjugate system which you will learn more about in the next section.\nSet up a new simulation using two lenses with focal lengths of 10 cm (objective lens) and 40 cm (tube lens). Place the object 20 cm in front of the first lens (the objective lens) and separate the lenses by a distance of 70 cm. You may assume that the diameter of the lenses is infinite for this example.\n\n\n3.0.2 Paper Calculation\nFirst, use basic ray tracing diagrams and calculations to predict the behavior of this system and answer the questions below.\n\nFor the first lens (objective lens) and intermediate image\n\nWhere is the image located?\nIs it real or virtual?\nUpright or inverted?\nWhat is its magnification?\n\nFor the second lens (tube lens) and final image\n\nWhere is the final image of the imaging system located?\nIs it real or virtual?\nUpright or inverted?\nWhat is its magnification (with respect to the initial object)?\n\nAre the rays between the two lenses converging, diverging, or traveling parallel?\n\n\n\n3.0.3 Computer Simulation\nNext you will simulate the system to confirm your calculations. We will follow steps similar to the tutorial in the Colab notebook.\n\nCreate variables to store the important properties of the system.\n\nobj_h to store the value of the object.\nf1 and f2 for the focal lengths of the objects\nlens_separation for the distance between the two lenses\ndiameter for the diameter of the lenses\n\nUse the ObjectRays() function to create the rays object with a diameter equal to the value specified in obj_h. You may choose any parameters that you would like for the half angle (halfAngle), number of rays bundles (H) and the number of rays per bundle (T). A good starting point is around halfAngle = 0.05 rad, H=2, T=5.\nCreate an ImagingPath object and add the lenses to the path.\nDisplay the imaging path.\n\nDo the results of your ray tracing simulation match your expectations? Why or why not? Upload a screenshot of your ray tracing simulation.\n\n\n3.0.4 Ray Tracing Analysis of an Infinite Conjugate System\nIn this section we will analyze an infinite conjugate system. An infinite conjugate system is the most common configuration for microscopes for reasons that we will see shortly. It is a subset of a type of optical system called a 4-f system. The 4-f system is a canonical optical system composed of two lenses separated by the sum of their focal lengths. In this section, you will simulate a 4-f system and investigate how different parameters in it affect its performance.\nGive the first lens a focal length of 10 cm and the second with a focal length of 40 cm. Separate the two lenses by a distance given by the sum of their focal lengths (50 cm) and place an aperture at the back focal plane of the first lens in your system (this will also by definition be at the front focal plane of your second lens). Place your object at the front focal plane of the first lens.\n\n\n3.0.5 Paper Calculation\nFirst, use basic ray tracing diagrams and calculations to predict the behavior of this system and answer the questions below.\n\nFor the first lens (objective lens) and intermediate image\n\nWhere is the image located?\nIs it real or virtual?\nUpright or inverted?\nWhat is its magnification?\n\nFor the second lens (tube lens) and final image\n\nWhere is the final image of the imaging system located?\nIs it real or virtual?\nUpright or inverted?\nWhat is its magnification (with respect to the initial object)?\n\nAre the rays between the two lenses converging, diverging, or traveling parallel?\n\n\n\n3.0.6 Computer Simulation\nSet up a new simulation with two lenses as in the section before. Launch a set of rays through the optical system to answer the following questions again for the infinite conjugate system:\n\nFor the first lens (objective lens) and intermediate image\n\nWhere is the image located?\nIs it real or virtual?\nUpright or inverted?\nWhat is its magnification?\n\nFor the second lens (tube lens) and final image\n\nWhere is the final image of the imaging system located?\nIs it real or virtual?\nUpright or inverted?\nWhat is its magnification (with respect to the initial object)?\n\nAre the rays between the two lenses converging, diverging, or traveling parallel?\n\n\n\n3.0.7 Aperture\nHow does the aperture at the back focal plane impact what rays can pass through the optical system? (Hint: create a large cone of rays and then see how changing its size impacts the bundle of rays)\nDo the results of your ray tracing simulation match your expectations? Why or why not? Upload a screenshot of your ray tracing simulation.\n\n\n3.0.8 The advantage of infinite conjugate systems\nIn this section we will investigate the advantages of an infinite conjugate optical system compared to a finite conjugate one. Keep the objects and distances between the lenses at the same distances as you set in the respective sections above.\nWe will compare the two systems by investigating what happens when we place a slab of glass into the optical path. Add a 5 cm thick slab of glass (\\(n=1.5\\)) in the space between the two lenses in both systems (making sure that you reduce the amount of space on either side to keep the lens separation the same). You can do this using the DielectricSlab object. Run the simulations again and comment on your observations for each system using the questions below.\n\nDoes the location of the image change?\nDoes the magnification of the system change?\nHow does the image location and magnification change as the thickness of the glass slab increases?\n\nBased on this analysis, why do you think that infinite conjugate systems are the preferred architecture in modern microscopes?\n\n\n\n\n\n\nHint\n\n\n\nConsider that it is often advantageous to put filters in the microscope system to, for example, block certain wavelength ranges."
  },
  {
    "objectID": "labs/lab2/index.html#raspberry-pi-camera-module",
    "href": "labs/lab2/index.html#raspberry-pi-camera-module",
    "title": "Introduction to Ray Tracing",
    "section": "4 Raspberry Pi Camera Module",
    "text": "4 Raspberry Pi Camera Module\n\n4.1 Raspberry Pi Camera Module Specs\nExplore the datasheet and specifications for the Raspberry Pi v2 camera here and fill in the table below.\n\n\n\nSpecification\nValue\nUnits\nNotes\n\n\n\n\nLens focal length\n\n\n\n\n\nLens f-number\n\n\n\n\n\nSensor pixel size (w x h)\n\n\n\n\n\nSensor size (w x h) in physical units\n\n\n\n\n\nPixel count (total number of pixels)\n\n\n\n\n\n\n\n\n4.2 Ray Tracing Simulation of the 3d-printed Raspberry Pi Microscope\nFor this section, you will perform a ray tracing simulation of the 3d-printed microscope architecture we will use in later labs. You may find a copy of the paper describing the system here.\nThis microscope is designed by removing the lens of the Raspberry Pi camera and displacing it from the sensor to create a microscope with a working distance of ~7 mm. Using the parameters of the camera and lens that you found from the datasheet and completed in the table above, set up a simulation to analyze this microscope configuration. Photos of the lens with a scale bar are provided below for your reference.\n\n\n\n\n\n\nFigure 1: Image of the lens from the Raspberry Pi Camera 2 Module.\n\n\n\n\n4.2.1 Camera Lens Questions\n\nEstimate the diameter of the lens from the photos.\nDoes this lens have a front or rear stop?\nEstimate the size of the stop from the photos. Then using the parameters from the lens data sheet that you collected in the table above, calculate the diameter of the stop. Do these values match?\n\n\n\n4.2.2 Simulation\nSet up a ray tracing simulation of the Raspberry Pi microscope using this single lens as a simple single-lens imaging system. Include any stops as appropriate. For example, you should consider the stop on the lens assembly and the finite size of the camera sensor. You may place the stop on the lens assembly at a distance of 1 mm from the lens. Then, use your simulation to answer the following questions.\n\nWrite an expression to calculate the lateral magnification of the optical system. (Hint: make sure you set the objectHeight property of the ImagingPath object and check out the imageSize function with the useObject parameter set to True.)\nHow far in front of the lens should the object be located in order to achieve a lateral magnification of 1.5x?\nAt this magnification of 1.5x, what is the image distance (i.e., how far behind the sensor is the image located)?\nWhat is the maximum object size that you can image? Note that the field of view is normally defined as the diameter of the object where half the cone of rays from the edge of the object passes through the optical system.\nWhat is the maximum angle that a ray emitted from the center of the object can have with respect to the optical axis such that it still passes through the optical system? This quantity is called the half-angle of the acceptance cone and is used in calculating the numerical aperture (NA) of the optical system using the expression. The NA is an important parameter for determining the resolution of the optical system. What is the NA of this optical system?\nHow would you change the optical system to achieve a magnification of 2x?\n\nSave a copy of your simulation figure to submit."
  },
  {
    "objectID": "labs/lab2/index.html#resources",
    "href": "labs/lab2/index.html#resources",
    "title": "Introduction to Ray Tracing",
    "section": "5 Resources",
    "text": "5 Resources\nDocumentation\n\nRayTracing\nPicamera\nRaspberry Pi Documentation - Camera"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Labs Overview",
    "section": "",
    "text": "Here is a listing of the labs available to explore."
  },
  {
    "objectID": "labs/index.html#hello-world",
    "href": "labs/index.html#hello-world",
    "title": "Labs Overview",
    "section": "1 Hello World",
    "text": "1 Hello World\nContains instructions on getting your Raspberry Pi configured and taking your first picture."
  },
  {
    "objectID": "labs/index.html#introduction-to-ray-tracing",
    "href": "labs/index.html#introduction-to-ray-tracing",
    "title": "Labs Overview",
    "section": "2 Introduction to Ray Tracing",
    "text": "2 Introduction to Ray Tracing\nThis lab contains instructions on performing ray tracing simulations on the design for the 3d-printed microscope platform."
  },
  {
    "objectID": "labs/index.html#d-printed-microscope-assembly",
    "href": "labs/index.html#d-printed-microscope-assembly",
    "title": "Labs Overview",
    "section": "3 3d-printed Microscope Assembly",
    "text": "3 3d-printed Microscope Assembly\nThis lab contains instructions on building and testing your 3d-printed microscope platform."
  },
  {
    "objectID": "labs/lab1/index.html",
    "href": "labs/lab1/index.html",
    "title": "Introduction to the Raspberry Pi and Raspberry Pi Camera Module",
    "section": "",
    "text": "By the end of this lab you will have…\n\nSet up an operating system on your Raspberry Pi and configured it for headless operation using ethernet over USB.\nSet up file sharing between the Pi and your host computer using Samba to allow for data to be easily transferred between the two systems.\nSet up Python in a virtual environment on your Pi to control data acquisition from the camera\nConnected a Raspberry Pi camera to you Pi and taken a series of images using both the libcamera command line tool and the Python package picamera\nPerformed basic image processing and analysis of the captured images in Python\n\nRemember to track the time you spend on this assignment, as you will be asked to submit it along with your work."
  },
  {
    "objectID": "labs/lab1/index.html#learning-objectives",
    "href": "labs/lab1/index.html#learning-objectives",
    "title": "Introduction to the Raspberry Pi and Raspberry Pi Camera Module",
    "section": "",
    "text": "By the end of this lab you will have…\n\nSet up an operating system on your Raspberry Pi and configured it for headless operation using ethernet over USB.\nSet up file sharing between the Pi and your host computer using Samba to allow for data to be easily transferred between the two systems.\nSet up Python in a virtual environment on your Pi to control data acquisition from the camera\nConnected a Raspberry Pi camera to you Pi and taken a series of images using both the libcamera command line tool and the Python package picamera\nPerformed basic image processing and analysis of the captured images in Python\n\nRemember to track the time you spend on this assignment, as you will be asked to submit it along with your work."
  },
  {
    "objectID": "labs/lab1/index.html#instructions",
    "href": "labs/lab1/index.html#instructions",
    "title": "Introduction to the Raspberry Pi and Raspberry Pi Camera Module",
    "section": "2 Instructions",
    "text": "2 Instructions\nThe first step is to get your Raspberry Pi set up. These labs will use the Raspberry Pi Zero 2 W and a Raspberry Pi Camera Module (specifically the NoIR version which has the IR shortpass filter removed to enable capturing light from a wider range of optical wavelengths.) The Raspberry Pi will be used as an external peripheral and controlled over the command line with ssh and through VNC if you prefer to use a graphical user interface (GUI). Don’t fear if you don’t have experience with the command line, this lab will provide you with a basic tutorial so that you can get up and running.\nThis lab has a series of steps. First, you will follow the procedure to set up the Raspberry Pi as an Ethernet Gadget. This leverages a convenient feature of the Pi that enables ethernet connections over the USB port. This will allow us to connect to the Pi over an ssh connection in the terminal to control the operation of the Pi. We will also set up file sharing over this network connection so that you have an easy way to transfer data from the Pi where it is captured, to your host computer which has more processing power and a display to be able to analyze and process your captured data.\nAfter setting up your Pi, you will walk through the process of installing your Raspberry Pi camera and learn how to take some test images with the libcamera command line tool. Following this, you will learn the basic features of picamera, the Python library that is used to interface with the camera. picamera gives you fine-grained control over the settings of the camera and offers the low-level control that is necessary for accessing the raw data captured by the sensor which is important for scientific applications like microscopy.\n\n2.1 Setup Raspberry Pi Zero as Ethernet Gadget\n\n2.1.1 Installing the Operating System\nIn this section, you will install an operating system on your Raspberry Pi and set it up so that you can control it in a headless configuration (i.e., without a dedicated external monitor) from your personal computer\nIf you don’t already have an SD card with an OS image installed, download the Raspberry Pi Imager and install the operating system onto your SD card. Click the button to choose an operating system and select the Raspberry Pi OS (32-bit) option which will download the latest version of Debian Linux for the Raspberry Pi (called Bullseye at the time of writing this guide). Then click the Choose Storage button and select the SD card you connected to your computer. After choosing the SD card, click the Write button to install the operating system.\n\n\n\n\n\n\nFigure 1: Raspberry Pi Imager screenshot\n\n\n\n\n\n2.1.2 Configuring the OS for Headless Operation\nAfter installing the OS, re-mount the SD card (most easily done by removing and then re-inserting the SD card). The new volume should be called “boot”. Next you will need to make a few changes to the files on the card to configure it for headless operation before continuing.\n\nOpen config.txt and append dtoverlay=dwc2 in a new line at the end of the file.\nOpen a terminal window (Terminal on MacOS/Linux/Unix or Powershell on Windows) and create a new file. Navigate to the boot volume and create a new file called ssh with no file extension using either touch ssh on Unix or echo &gt;&gt; ssh on Windows. (You may also create a new file with a text editor like VS Code and save it with the name ssh, again without a file extension).\nFinally, open the file called cmdline.txt, look for the text rootwait and add the following right after it modules-load=dwc2,g_ether. The formatting of cmdline.txt is very strict; commands are separated by spaces. Be sure not to add any additional newline characters.\n\nNow eject the SD card, install it in the Pi, and connect your Pi to your computer via USB using the port labeled USB (not PWR_IN!)\n\n\n\n2.2 Connecting to the Raspberry Pi Using SSH\nAfter connecting it to your computer, you should see the activity LED light up (note there is no power LED on the Pi Zero) and after it finishes booting, you should see the Pi show up as a new network interface with a self-assigned IP address.\nWe will be communicating with our Pi over ssh or Secure Shell. ssh is a very popular cryptographic network protocol which is used to communicate securely over an unsecured network.\nAfter you see the Pi get an IP address, you should be able to ssh into the Pi in terminal. Open your terminal emulator up again and type ssh pi@raspberrypi.local or ssh pi@&lt;ip_addr&gt; where &lt;ip_addr&gt; is the IP address you saw the Pi was assigned in the previous step. raspberrypi.local is a dynamic nameserver (DNS) shortcut which redirects to the IP for the Pi and is a handy shortcut to save you the step of looking up the actual assigned address.\nAfter typing this command you should get a message back that the connection has been made and if it is the first time you ssh in, you will also get a prompt asking you to add the Pi to the list of known hosts. (Note: If you have ever sshed into a different rasperrypi.local device you may see an error about a duplicate in your ssh known_hosts. To fix this, navigate to the known_hosts file and remove the line corresponding to raspberrypi.local.)\nYou will then get a prompt for the password for the pi user which is raspberry by default. After entering it, you should see a message that you have been logged in. Using the default password is a security hazard, so the first thing you should do is set a new one. Enter passwd at the command line prompt and follow the prompts to set a new login password for the user pi.\n\n\n2.3 Connecting to the Internet\nThere are two ways to connect to the Internet from the Pi. You may either connect directly to WiFi or you can share the Internet connection from your host computer. You may choose either option and follow the instructions below to set it up.\n\n\n2.4 Connecting to WiFi Directly\nTo connect to WiFi directly, run sudo raspi-config and then navigate to 1 System Options &gt; S1 Wireless LAN and enter your SSID and password to connect.\n\n\n2.5 Sharing Internet from Host Machine Over USB\nTo get internet on your Pi we will share the network connection from your host machine over the USB connection. To do this, To do this, edit the /etc/network/interfaces file. You’ll need to edit it with sudo (i.e., admin) privileges. An easy way to do this is with the nano text editor. To open the file to edit, type sudo nano /etc/network/interfaces at your Pi’s command prompt (over ssh).\nModify the file to have the following contents\n# Setup USB internet\nauto lo usb0 \nallow-hotplug usb0\niface usb0 inet manual\nWhen you are finished, the file should look like this.\n\n\n\n\n\n\nFigure 2: Edited /etc/network/interfaces/ file\n\n\n\nWhen you finish editing the file, exit out of the file saving changes using ctrl + x and then reboot your Pi by typing sudo reboot.\nNow that the Pi is configured to connect to the internet over USB, you need to configure your host to share its internet connection. Follow the instructions here to configure your host. After you configure it, test that it is working properly by trying to ping a website like google from your Pi (e.g., by running ping google.com) from the command line.\nAfter setting up the internet connection, update your packages by running sudo apt-get update and responding y to any prompts.\n\n\n2.6 Enable and Setup Virtual Network Computing (VNC) Interface\nTo enable the VNC interface, first run the raspi-config tool by typing sudo raspi-config at the prompt on your Pi. Then navigate to option 3 Interface Options and select option I3 VNC. Make sure that &lt;Yes&gt; is selected. After exiting the raspi-config interface you may need to reboot to see changes take effect (you can do so by typing sudo reboot.\nYou will also need to install a VNC viewer on your host computer. A popular free option is VNC Viewer from RealVNC available for download here.\nAfter downloading VNC Viewer and enabling VNC on your Pi, you can connect to a virtual desktop. Open VNC Viewer and type the IP address of your Pi (e.g., raspberrypi.local) in the address bar at the top of the window. Press enter and you should see a window showing the Raspberry Pi desktop.\nWhile you are configuring the VNC, you should also configure the VNC server to enable direct capture mode. This is important for viewing some windows (the libcamera preview window is one important example) over VNC. To do this, on the Raspberry Pi desktop click on VNC icon in the top right of the menubar. Then click on the icon with 3 horizontal bars at the top right of the VNC Server window and select Options. In the options window, select Troubleshooting and then check the checkbox Enable direct capture mode. Click OK to apply the changes and close the window.\nYou may also want to configure the VNC resolution if it does not look as you wish on your host. You can change these settings in the raspi-config utility under menu option 2 Display Options.\n\n\n2.7 Setup Samba Share\nNext we will configure Server Message Block (SMB) or Samba file sharing from the Pi. This will enable you to share a folder on the Raspberry Pi so that you can easily transfer data between the Pi and your host computer using a file browser such as Finder on MacOS or Explorer on Windows.\nTo set up the sharing, we will use the aptitude package manager apt. The first step is to update the package manager by running sudo apt-get update and after that finishes running sudo apt-get upgrade. After both commands finish, install Samba packages by running sudo apt-get install samba samba-common-bin (You may get a message about changing the DHCP settings. You should be able to choose either option.)\nThen create a new folder in your home directory to store shared files by running mkdir /home/pi/shared.\nNow we need to set up Samba to share this folder over the network. First open the Samba configuration file by typing sudo nano /etc/samba/smb.conf. This will open the file in nano which is a command line text editor. Scroll to the bottom and add the following lines to the file.\n[CameraSTEMsShared]\npath = /home/pi/shared\nwriteable=Yes\ncreate mask=0777\ndirectory mask=0777\npublic=no\nNext, set up a new user for accessing the Samba share. Run sudo smbpasswd -a pi and then enter a password. This password can be distinct from your user password.\nFinally, restart the samba service by running sudo systemctl restart smbd.\nNow that configuration is done on the Pi, the final step is to set up access from your host.\n\nOn Mac, go to Finder and navigate to Go \\&gt; Connect to Server. Then connect to the server smb://&lt;hostname_ip&gt;/CameraSTEMsShared where &lt;hostname_ip&gt; is the IP address of your Pi. You can either use the raspberrypi.local dynamic address or find the current IP address of your Pi by running hostname -I on the Pi. Login with the username pi and the password you set for the samba share.\nOn Windows, open Explorer and click on the Computer tab at the top of the Window. On the top bar, click Map network drive. Then in the Folder path field, enter \\&lt;hostname_ip\\&gt;\\CameraSTEMsShared. Make sure to tick Connect using different credentials so that it prompts you for the username and password you set up. When you enter your username, you’ll need to use a backslash before your user name (e.g. \\pi) so that it does not try to login on the workspace of your host computer. After clicking Finish then enter your user details and click ok.\n\n\n\n2.8 Setting up Python Virtual Environment\nIn this section, you will set up a Python virtual environment. Virtual environments enable you to set up an isolated environment containing only specific packages and versions without requiring you to have these packages installed globally. This avoids conflicts and compatibility issues with different packages.\nTo set up the virtual environment, navigate to your /home/pi/shared directory and run ​​python3 -m venv /home/pi/shared/.env. This will create a new virtual environment and store the files within the folder .env. The dot preceding the folder name means that it is hidden which is helpful to avoid cluttering your terminal output. (Hint: you can see all files in a directory by running the list command ls with the -a flag which stands for all).\nAfter creating your virtual environment, we need to activate it. To do this navigate to /home/pi/shared and then run source .env/bin/activate. This will run a shell script which will reconfigure your path variables to point to the version of Python and the packages installed in your environment as opposed to those installed on your system by default. After you activate the environment, you should see the name of the environment listed in parentheses at the left of the command prompt (e.g., (.env) in our case). To deactivate your virtual environment, enter deactivate at the command prompt.\nTo confirm that your virtual environment is working properly, deactivate the environment and run the command which python3. The which command returns the path where a command is located. This should return /usr/bin/python3 which is the standard location for the Python binary on our system. Then, activate your virtual environment and run which python3 again. Now you should see that the path to python3 is now changed to point to the version of python which is installed in our virtual environment.\n\n\n\n\n\n\nFigure 3: Virtual environment setup.\n\n\n\n\n\n2.9 Installing the Camera\n\n2.9.1 Enable Camera Interface\nBy default the camera interface on your Pi may not be enabled. To make sure it is turned on, use the raspi-config utility to enable it. At the command prompt type sudo raspi-config. Then navigate to the Interfacing Options sub-menu and select the appropriate option to enable the camera. Then shutdown your Pi by running sudo halt, waiting a minute for the LED to turn off, and disconnecting it from the USB cable.\n\n\n2.9.2 Connecting the Camera to Your Pi\nNext, you will connect the Raspberry Pi NoIR camera module to your Pi. Take out the camera module and the camera cable. Then connect the small end of the cable to the camera interface on your Pi and the other wider end of the cable to your camera module. You may need to gently pull the black plastic parts of the receptacles on the Pi and the camera PCB so that you can insert the cable. Then insert the camera cable. Note that the orientation of the cable matters. On both sides, the contacts of the cable (i.e., the metal strips where the connection is made) should be oriented so that they face toward the PCBs. After you firmly seat the cable into the receptacle, gently push the black plastic piece in toward the connector to lock the ribbon cable in. Now your camera is connected and you are ready to take your first picture. Plug the power cable back in and boot your Pi up before continuing to the next section.\n\n\n\n2.10 Take Your First Picture\nIn this section you will take your first image with your camera. We will be using the Python library picamera to control the Raspberry Pi camera and to capture and save images. In this section we will walk through how to install the package in your virtual environment and take your first picture!\n\n2.10.1 Install picamera\nOnce you have your virtual environment set up, now we want to install the picamera library which will enable us to access and control our camera from Python. To do this, we’ll use the Python package manager pip. First, activate your environment. Then, run pip3 list. This will print out a list of all the packages installed in your environment. You should see a list of the default packages installed in our environment (probably just pip, pkg-resources, setuptools). To install picamera, run pip3 install picamera (Note: you can also use pip instead of pip3 but it is safer to use pip3 to ensure that the packages are installed for Python 3 instead of Python 2).\nAfter running this command, run pip3 list. You should see that picamera has been added to the list of installed modules.\n\n\n2.10.2 Take Your First Image\nNow let’s use picamera to take an image. Open a new Python interpreter by running python3. Then import picamera by running import picamera. Then create a PiCamera object called cam by running cam = picamera.PiCamera().\nNow you can take an image using the capture method that is a member of the PiCamera class (and thus our PiCamera object cam). To save an image named hello_raspberry.jpg run cam.capture(‘hello_raspberry.jpg’). In your host OS, navigate to the shared folder and you should see a new saved image file. Open it and check it out.\nNow, of course you don’t want to have to manually type commands one by one at the command line. Create a new file (the easiest way to do this is to type touch &lt;file_name&gt; on your Pi). Then you can edit it either in the terminal using your favorite terminal text editor or by opening the file on your host OS via the mounted SMB shared network drive. Type the code below into a file called save_image_to_file.py and save it. Then run the script from the command line by running python3 save_image_to_file.py. You should see\n# Import libraries\nfrom time import sleep\nfrom picamera import PiCamera\n\n# Create camera object\ncam = PiCamera()\n# Camera warm-up time\nsleep(2)\nprint(\"Camera is ready!\")\n# Capture the camera image and save it to disk\ncam.capture('hello_raspberry.jpg')\nprint(\"Photo captured and saved.\")\n# Close the camera object\ncam.close()\nCode listing for save_image_to_file.py\n\n\n\n2.11 Process Image in Python\nNext we will load the image in Python to do some simple analysis and inspection. Since the Raspberry Pi has limited computational power compared to your host machine, it makes sense to do processing on your host OS. For this course we will do all of our programming in Python. You can use any Python environment that you would like (e.g., raw scripts run from terminal, PyCharm, IPython notebooks either locally or on Colab, etc.). In this section we will go through setting up Spyder which is a development environment for Python which features a very similar layout and feel to Matlab.\n\n2.11.1 Download Spyder\nFirst, navigate to the website for Spyder and download the installer. After installing the program, open it up. You will be presented with the home view. (Note: On macOS you may need to navigate to your Applications folder and launch Sypder by right clicking on it and clicking Open when opening it for the first time to get around the operating system’s gatekeeper functionality)"
  },
  {
    "objectID": "labs/lab1/index.html#image-acquisition-and-analysis-experiments",
    "href": "labs/lab1/index.html#image-acquisition-and-analysis-experiments",
    "title": "Introduction to the Raspberry Pi and Raspberry Pi Camera Module",
    "section": "3 Image Acquisition and Analysis Experiments",
    "text": "3 Image Acquisition and Analysis Experiments\nAfter you have finished setting up the image acquisition and analysis pipeline, in this section you will use your system to run a few experiments to analyze different performance specifications of the Raspberry Pi Camera. The suggested workflow is to write your code on an editor on your host machine and then run it on your Pi via ssh. For example, you could create a new Python script saved in the directory shared via Samba. Then, on your Pi you can run the script from the command line (e.g., with by running python3 \\&lt;script_name\\&gt; in the shared directory) and save your data. Finally, you can then load the files on your host machine in Spyder to do the plotting and analysis required.\nThe following sections list a series of tasks for you to accomplish and submit in your lab writeup.\n\n3.1 Image with Scale Bar\nTake an image of an object and plot it in Python using matplotlib and the imshow function. In your plot, make sure to label the axes along with units, give your plot a descriptive title, and use a colorbar.\nThe recommended method for creating plots in Python is to use the subplots function to generate a figure and axes object which can be manipulated after it is created as shown in the example code snippet below.\nimport matplotlib.pyplot as plt           # import matplotlib\nfig, ax = plt.subplots(nrows=1, ncols=1)  # create subplots\nim_data = plt.imread(\"test_image.jpg\")    # load image\nhim = ax.imshow(im_data)                  # plot image and save handle\nWhenever you are plotting images it is critical to visually indicating the scale of the objects in the photo. One way to do this is to directly calculate the magnification of the object and then use this information to convert pixel number to a physical length in the image. A simpler way is to measure the size in pixels of an object of known length in your image and use this to calculate the size of a pixel in your image in physical units of length such as meters or centimeters.\nFor your image, use something that is of known length in your image to calculate the le ngth of the object in pixels. Then, plot a rectangle on top of the image by creating a rectangle patch and adding it to your axis. See the link here for an example. Then add text to your image above the scale bar to indicate its length. You may find the documentation for the text function provided in matplotlib helpful.\nThe image below provides an example of what an image with scale bar should look like.\n\n\n\n\n\n\nFigure 4: An example of a scale bar on a microscope image. From: https://doi.org/10.1038/s41566-017-0078-z\n\n\n\n\n\n3.2 Raw Bayer Image Capture\nColor camera sensors are typically formed by putting an array of red, green, and blue filters over the monochrome pixels. A popular configuration is the Bayer color filter array (CFA). In this section, you will take and save the raw Bayer data from the sensor. Read the PiCamera documentation here on the PiBayerDataArray data structure and use it to capture a Bayer array and save to file. An easy way to save the data is to use the numpy function save to save the raw data to an .npy file. Then the raw data can be reloaded on your host computer using the corresponding load function in numpy.\nIn your analysis, plot each color channel of the image separately (e.g., separate plots for the red, green, and blue channels) and answer the following questions.\n\nWhat is the size of the Bayer image data structure?\nWhat does each dimension represent?\nWhich channel of your captured image is the brightest? Is this what you expect? Why or why not?\n\n\n\n3.3 Dark Noise Analysis\nThe dark noise value is the value of the image data captured when the sensor is covered (i.e., not exposed to light). Cover your sensor and capture a series of 10 dark frames and answer the following questions. Plot the average image (i.e., 2D image where each pixel is an average of its values across the frames). Then for the red channel of the image only answer the following questions.\n\nWhat is the mean value of the red channel of your image sequence?\nWhat is the standard deviation of this array of means calculated from your image sequence?\n\n\n\n3.4 Image Intensity vs. Exposure Time\nAnother important property of a camera is its linearity. In this section, you will take an exposure series and analyze how the average intensity of the sensor varies with exposure time. See the documentation here which will help provide information on how to take consistent images by disabling any automatic exposure compensation and directly setting the exposure time (called shutter_speed in picamera).\nUncover your sensor and take a set of images across a variety of exposure times. Take 10 images at each exposure time and vary the exposure time over three orders of magnitude (e.g., from 1 ms to 1 second) with at least 5 different exposure times. Then take the average of the images at each exposure time and the standard deviation of these average intensities. Then, plot the means vs. exposure time on a linear plot with error bars to indicate the standard deviation of the mean values calculated at each exposure time (hint: check out the errorbar plotting function in matplotlib). Fit the data with a line and report back both the slope of the line and its \\(R^2\\) value. You may find this tutorial on curve fitting in Python helpful."
  },
  {
    "objectID": "labs/lab1/index.html#faqs",
    "href": "labs/lab1/index.html#faqs",
    "title": "Introduction to the Raspberry Pi and Raspberry Pi Camera Module",
    "section": "4 FAQs",
    "text": "4 FAQs\n\n\n\n\n\n\nEnabling virtual enviornment\n\n\n\n\n\nRun source .env/bin/activate to enable virtual environment and deactivate to disable.\n\n\n\n\n\n\n\n\n\nGetting an error when trying to run a script\n\n\n\n\n\nMake sure you are running things on the right machine (host vs. Pi)"
  },
  {
    "objectID": "labs/lab2/code/Intro_to_RayTracing.html",
    "href": "labs/lab2/code/Intro_to_RayTracing.html",
    "title": "General Workflow",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Install raytracing module using pip\n!pip install raytracing\n\nCollecting raytracing\n  Downloading raytracing-1.3.7-py3-none-any.whl (1.6 MB)\n     |▏                               | 10 kB 15.9 MB/s eta 0:00:01     |▍                               | 20 kB 16.0 MB/s eta 0:00:01     |▋                               | 30 kB 18.8 MB/s eta 0:00:01     |▉                               | 40 kB 14.7 MB/s eta 0:00:01     |█                               | 51 kB 14.3 MB/s eta 0:00:01     |█▏                              | 61 kB 15.5 MB/s eta 0:00:01     |█▍                              | 71 kB 14.7 MB/s eta 0:00:01     |█▋                              | 81 kB 15.9 MB/s eta 0:00:01     |█▉                              | 92 kB 15.2 MB/s eta 0:00:01     |██                              | 102 kB 16.2 MB/s eta 0:00:01     |██▎                             | 112 kB 16.2 MB/s eta 0:00:01     |██▍                             | 122 kB 16.2 MB/s eta 0:00:01     |██▋                             | 133 kB 16.2 MB/s eta 0:00:01     |██▉                             | 143 kB 16.2 MB/s eta 0:00:01     |███                             | 153 kB 16.2 MB/s eta 0:00:01     |███▎                            | 163 kB 16.2 MB/s eta 0:00:01     |███▍                            | 174 kB 16.2 MB/s eta 0:00:01     |███▋                            | 184 kB 16.2 MB/s eta 0:00:01     |███▉                            | 194 kB 16.2 MB/s eta 0:00:01     |████                            | 204 kB 16.2 MB/s eta 0:00:01     |████▎                           | 215 kB 16.2 MB/s eta 0:00:01     |████▌                           | 225 kB 16.2 MB/s eta 0:00:01     |████▋                           | 235 kB 16.2 MB/s eta 0:00:01     |████▉                           | 245 kB 16.2 MB/s eta 0:00:01     |█████                           | 256 kB 16.2 MB/s eta 0:00:01     |█████▎                          | 266 kB 16.2 MB/s eta 0:00:01     |█████▌                          | 276 kB 16.2 MB/s eta 0:00:01     |█████▊                          | 286 kB 16.2 MB/s eta 0:00:01     |█████▉                          | 296 kB 16.2 MB/s eta 0:00:01     |██████                          | 307 kB 16.2 MB/s eta 0:00:01     |██████▎                         | 317 kB 16.2 MB/s eta 0:00:01     |██████▌                         | 327 kB 16.2 MB/s eta 0:00:01     |██████▊                         | 337 kB 16.2 MB/s eta 0:00:01     |██████▉                         | 348 kB 16.2 MB/s eta 0:00:01     |███████                         | 358 kB 16.2 MB/s eta 0:00:01     |███████▎                        | 368 kB 16.2 MB/s eta 0:00:01     |███████▌                        | 378 kB 16.2 MB/s eta 0:00:01     |███████▊                        | 389 kB 16.2 MB/s eta 0:00:01     |████████                        | 399 kB 16.2 MB/s eta 0:00:01     |████████                        | 409 kB 16.2 MB/s eta 0:00:01     |████████▎                       | 419 kB 16.2 MB/s eta 0:00:01     |████████▌                       | 430 kB 16.2 MB/s eta 0:00:01     |████████▊                       | 440 kB 16.2 MB/s eta 0:00:01     |█████████                       | 450 kB 16.2 MB/s eta 0:00:01     |█████████▏                      | 460 kB 16.2 MB/s eta 0:00:01     |█████████▎                      | 471 kB 16.2 MB/s eta 0:00:01     |█████████▌                      | 481 kB 16.2 MB/s eta 0:00:01     |█████████▊                      | 491 kB 16.2 MB/s eta 0:00:01     |██████████                      | 501 kB 16.2 MB/s eta 0:00:01     |██████████▏                     | 512 kB 16.2 MB/s eta 0:00:01     |██████████▎                     | 522 kB 16.2 MB/s eta 0:00:01     |██████████▌                     | 532 kB 16.2 MB/s eta 0:00:01     |██████████▊                     | 542 kB 16.2 MB/s eta 0:00:01     |███████████                     | 552 kB 16.2 MB/s eta 0:00:01     |███████████▏                    | 563 kB 16.2 MB/s eta 0:00:01     |███████████▍                    | 573 kB 16.2 MB/s eta 0:00:01     |███████████▌                    | 583 kB 16.2 MB/s eta 0:00:01     |███████████▊                    | 593 kB 16.2 MB/s eta 0:00:01     |████████████                    | 604 kB 16.2 MB/s eta 0:00:01     |████████████▏                   | 614 kB 16.2 MB/s eta 0:00:01     |████████████▍                   | 624 kB 16.2 MB/s eta 0:00:01     |████████████▌                   | 634 kB 16.2 MB/s eta 0:00:01     |████████████▊                   | 645 kB 16.2 MB/s eta 0:00:01     |█████████████                   | 655 kB 16.2 MB/s eta 0:00:01     |█████████████▏                  | 665 kB 16.2 MB/s eta 0:00:01     |█████████████▍                  | 675 kB 16.2 MB/s eta 0:00:01     |█████████████▋                  | 686 kB 16.2 MB/s eta 0:00:01     |█████████████▊                  | 696 kB 16.2 MB/s eta 0:00:01     |██████████████                  | 706 kB 16.2 MB/s eta 0:00:01     |██████████████▏                 | 716 kB 16.2 MB/s eta 0:00:01     |██████████████▍                 | 727 kB 16.2 MB/s eta 0:00:01     |██████████████▋                 | 737 kB 16.2 MB/s eta 0:00:01     |██████████████▉                 | 747 kB 16.2 MB/s eta 0:00:01     |███████████████                 | 757 kB 16.2 MB/s eta 0:00:01     |███████████████▏                | 768 kB 16.2 MB/s eta 0:00:01     |███████████████▍                | 778 kB 16.2 MB/s eta 0:00:01     |███████████████▋                | 788 kB 16.2 MB/s eta 0:00:01     |███████████████▉                | 798 kB 16.2 MB/s eta 0:00:01     |████████████████                | 808 kB 16.2 MB/s eta 0:00:01     |████████████████▏               | 819 kB 16.2 MB/s eta 0:00:01     |████████████████▍               | 829 kB 16.2 MB/s eta 0:00:01     |████████████████▋               | 839 kB 16.2 MB/s eta 0:00:01     |████████████████▉               | 849 kB 16.2 MB/s eta 0:00:01     |█████████████████               | 860 kB 16.2 MB/s eta 0:00:01     |█████████████████▏              | 870 kB 16.2 MB/s eta 0:00:01     |█████████████████▍              | 880 kB 16.2 MB/s eta 0:00:01     |█████████████████▋              | 890 kB 16.2 MB/s eta 0:00:01     |█████████████████▉              | 901 kB 16.2 MB/s eta 0:00:01     |██████████████████              | 911 kB 16.2 MB/s eta 0:00:01     |██████████████████▎             | 921 kB 16.2 MB/s eta 0:00:01     |██████████████████▍             | 931 kB 16.2 MB/s eta 0:00:01     |██████████████████▋             | 942 kB 16.2 MB/s eta 0:00:01     |██████████████████▉             | 952 kB 16.2 MB/s eta 0:00:01     |███████████████████             | 962 kB 16.2 MB/s eta 0:00:01     |███████████████████▎            | 972 kB 16.2 MB/s eta 0:00:01     |███████████████████▍            | 983 kB 16.2 MB/s eta 0:00:01     |███████████████████▋            | 993 kB 16.2 MB/s eta 0:00:01     |███████████████████▉            | 1.0 MB 16.2 MB/s eta 0:00:01     |████████████████████            | 1.0 MB 16.2 MB/s eta 0:00:01     |████████████████████▎           | 1.0 MB 16.2 MB/s eta 0:00:01     |████████████████████▌           | 1.0 MB 16.2 MB/s eta 0:00:01     |████████████████████▋           | 1.0 MB 16.2 MB/s eta 0:00:01     |████████████████████▉           | 1.1 MB 16.2 MB/s eta 0:00:01     |█████████████████████           | 1.1 MB 16.2 MB/s eta 0:00:01     |█████████████████████▎          | 1.1 MB 16.2 MB/s eta 0:00:01     |█████████████████████▌          | 1.1 MB 16.2 MB/s eta 0:00:01     |█████████████████████▋          | 1.1 MB 16.2 MB/s eta 0:00:01     |█████████████████████▉          | 1.1 MB 16.2 MB/s eta 0:00:01     |██████████████████████          | 1.1 MB 16.2 MB/s eta 0:00:01     |██████████████████████▎         | 1.1 MB 16.2 MB/s eta 0:00:01     |██████████████████████▌         | 1.1 MB 16.2 MB/s eta 0:00:01     |██████████████████████▊         | 1.1 MB 16.2 MB/s eta 0:00:01     |██████████████████████▉         | 1.2 MB 16.2 MB/s eta 0:00:01     |███████████████████████         | 1.2 MB 16.2 MB/s eta 0:00:01     |███████████████████████▎        | 1.2 MB 16.2 MB/s eta 0:00:01     |███████████████████████▌        | 1.2 MB 16.2 MB/s eta 0:00:01     |███████████████████████▊        | 1.2 MB 16.2 MB/s eta 0:00:01     |████████████████████████        | 1.2 MB 16.2 MB/s eta 0:00:01     |████████████████████████        | 1.2 MB 16.2 MB/s eta 0:00:01     |████████████████████████▎       | 1.2 MB 16.2 MB/s eta 0:00:01     |████████████████████████▌       | 1.2 MB 16.2 MB/s eta 0:00:01     |████████████████████████▊       | 1.2 MB 16.2 MB/s eta 0:00:01     |█████████████████████████       | 1.3 MB 16.2 MB/s eta 0:00:01     |█████████████████████████       | 1.3 MB 16.2 MB/s eta 0:00:01     |█████████████████████████▎      | 1.3 MB 16.2 MB/s eta 0:00:01     |█████████████████████████▌      | 1.3 MB 16.2 MB/s eta 0:00:01     |█████████████████████████▊      | 1.3 MB 16.2 MB/s eta 0:00:01     |██████████████████████████      | 1.3 MB 16.2 MB/s eta 0:00:01     |██████████████████████████▏     | 1.3 MB 16.2 MB/s eta 0:00:01     |██████████████████████████▎     | 1.3 MB 16.2 MB/s eta 0:00:01     |██████████████████████████▌     | 1.3 MB 16.2 MB/s eta 0:00:01     |██████████████████████████▊     | 1.4 MB 16.2 MB/s eta 0:00:01     |███████████████████████████     | 1.4 MB 16.2 MB/s eta 0:00:01     |███████████████████████████▏    | 1.4 MB 16.2 MB/s eta 0:00:01     |███████████████████████████▍    | 1.4 MB 16.2 MB/s eta 0:00:01     |███████████████████████████▌    | 1.4 MB 16.2 MB/s eta 0:00:01     |███████████████████████████▊    | 1.4 MB 16.2 MB/s eta 0:00:01     |████████████████████████████    | 1.4 MB 16.2 MB/s eta 0:00:01     |████████████████████████████▏   | 1.4 MB 16.2 MB/s eta 0:00:01     |████████████████████████████▍   | 1.4 MB 16.2 MB/s eta 0:00:01     |████████████████████████████▌   | 1.4 MB 16.2 MB/s eta 0:00:01     |████████████████████████████▊   | 1.5 MB 16.2 MB/s eta 0:00:01     |█████████████████████████████   | 1.5 MB 16.2 MB/s eta 0:00:01     |█████████████████████████████▏  | 1.5 MB 16.2 MB/s eta 0:00:01     |█████████████████████████████▍  | 1.5 MB 16.2 MB/s eta 0:00:01     |█████████████████████████████▋  | 1.5 MB 16.2 MB/s eta 0:00:01     |█████████████████████████████▊  | 1.5 MB 16.2 MB/s eta 0:00:01     |██████████████████████████████  | 1.5 MB 16.2 MB/s eta 0:00:01     |██████████████████████████████▏ | 1.5 MB 16.2 MB/s eta 0:00:01     |██████████████████████████████▍ | 1.5 MB 16.2 MB/s eta 0:00:01     |██████████████████████████████▋ | 1.5 MB 16.2 MB/s eta 0:00:01     |██████████████████████████████▊ | 1.6 MB 16.2 MB/s eta 0:00:01     |███████████████████████████████ | 1.6 MB 16.2 MB/s eta 0:00:01     |███████████████████████████████▏| 1.6 MB 16.2 MB/s eta 0:00:01     |███████████████████████████████▍| 1.6 MB 16.2 MB/s eta 0:00:01     |███████████████████████████████▋| 1.6 MB 16.2 MB/s eta 0:00:01     |███████████████████████████████▉| 1.6 MB 16.2 MB/s eta 0:00:01     |████████████████████████████████| 1.6 MB 16.2 MB/s eta 0:00:01     |████████████████████████████████| 1.6 MB 16.2 MB/s \nRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from raytracing) (1.19.5)\nRequirement already satisfied: matplotlib&gt;=3 in /usr/local/lib/python3.7/dist-packages (from raytracing) (3.2.2)\nRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from raytracing) (2.6.1)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3-&gt;raytracing) (1.3.2)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3-&gt;raytracing) (3.0.7)\nRequirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3-&gt;raytracing) (2.8.2)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3-&gt;raytracing) (0.11.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib&gt;=3-&gt;raytracing) (1.15.0)\nInstalling collected packages: raytracing\nSuccessfully installed raytracing-1.3.7\n\n\n\nfrom raytracing import *\n\nThe general workflow for simulating an optical system is as follows: 1. Create the ImagingPath() object 2. Create an ObjectRays() object with desired parameters (e.g., number of rays, angular range, etc.) 3. Add optical components to the imaging path 4. Display the path, making sure to pass the rays object to the display function 5. Print out any desired information about the imaging path using the appropriate methods of the ImagingPath object. See documentation on ImagingPath here for details.\n\n# Create imaging path object\npath = ImagingPath()\n# Create rays object\n# Here we have set:\n#  - The diameter of the object to 6\n#  - the half angle to 0.01 rad\n#  - the number of point sources or the number of ray fans (H) to 3\n#  - the number of rays in each ray fan (T) to 3\nobjRays = ObjectRays(diameter=5, halfAngle=0.01, H=3, T=3)\n\n# Add/append objects to the imaging path. Use Space() to generate a gap between objects\npath.append(Space(d=50))\npath.append(Aperture(diameter=20))\npath.append(Space(d=50,n=3))\npath.append(Lens(f=50, diameter=10))\npath.append(Space(d=200))\npath.display(rays=objRays, removeBlocked=False, limitObjectToFieldOfView=False)\n\n# Print out desired information about the optical system\n\nprint(path.fieldOfView())\nprint(f\"Image size is {path.imageSize(useObject=True)}\")\n\n\n\n\n\n\n\n\n39.99999692719027\nImage size is 10.0\n\n\n\n1 Helpful Notes\n\nRead the documentation thoroughly if you have questions!\nRays which are blocked as they pass through the imaging system and don’t contribute to the final image are left off the displayed diagram by default. To change this, pass the argument removeBlocked=False to the display() method of the ImagingPath object.\n\n\npath = ImagingPath()\nobjRays = ObjectRays(diameter=20, halfAngle=0.05, H=3, T=3)\n\npath.append(Space(d=50))\npath.append(Lens(f=50, diameter=15))\npath.append(Space(d=200))\npath.append(Lens(f=50))\npath.append(Space(d=100))\npath.display(rays=objRays)\n\n\n/usr/local/lib/python3.7/dist-packages/raytracing/figure.py: 117\nBeginnerHint: Infinite field of view: cannot use limitObjectToFieldOfView=True. The object height is instead set to the default value of 10.0.\n\n/usr/local/lib/python3.7/dist-packages/raytracing/imagingpath.py: 797\nBeginnerHint: Field of view is infinite. You can pass useObject=True to use the finite objectHeight.\n\n/usr/local/lib/python3.7/dist-packages/raytracing/figure.py: 134\nBeginnerHint: No aperture stop in the system: cannot use onlyPrincipalAndAxialRays=True since they are not defined. Showing the default ObjectRays instead. \n\n\n\n\n\n\n\n\n\n\npath = ImagingPath()\n#rays = UniformRays(yMax=10,M=3,N=3)\nrays = ObjectRays(diameter=10, halfAngle=0.1)\npath.append(Space(d=100))\npath.append(Lens(f=50, diameter=30))\npath.append(Space(d=30))\npath.append(Aperture(diameter=15))\npath.append(Space(d=170))\npath.display(rays,removeBlocked=False)\nprint(path.NA())\n\n\n\n\n\n\n\n\n0.1069379820609651"
  },
  {
    "objectID": "labs/lab3/index.html",
    "href": "labs/lab3/index.html",
    "title": "3D-printed Microscope Build",
    "section": "",
    "text": "By the end of this lab you will have…\n\nBuilt your own microscope\nLearned how to control the LED array from the Raspberry Pi\nCaptured an image and analyzed it to determine the resolution of the optical system."
  },
  {
    "objectID": "labs/lab3/index.html#learning-objectives",
    "href": "labs/lab3/index.html#learning-objectives",
    "title": "3D-printed Microscope Build",
    "section": "",
    "text": "By the end of this lab you will have…\n\nBuilt your own microscope\nLearned how to control the LED array from the Raspberry Pi\nCaptured an image and analyzed it to determine the resolution of the optical system."
  },
  {
    "objectID": "labs/lab3/index.html#setup",
    "href": "labs/lab3/index.html#setup",
    "title": "3D-printed Microscope Build",
    "section": "2 Setup",
    "text": "2 Setup\nIn this section you will put together the individual components for your microscope that are in your kit. This consists of printing the mount, soldering the LED array and Raspberry Pi headers, and assembling the parts with the screws and springs included in your kit.\nThe microscope you will build is based on the microscope developed by Tomas Aidukas, Regina Eckert, Andrew R. Harvey, Laura Waller, and Pavan C. Konda in their 2019 paper “Low-cost, sub-micron resolution, wide-field computational microscopy using opensource hardware“. You can read the paper here and access the supplementary information here.\n\n\n\n\n\n\nFigure 1: A photo of the finished 3D-printed microscope.\n\n\n\n\n2.1 Makerspace 3D Printer Room Access\nBefore printing the parts for your microscope, you’ll need to get access to the 3D printing room in the Makerspace if you haven’t done so already. You will need to read two policies and take the associated quizzes: the General Use Policy and the 3D printer policy. After finishing the quizzes, you should shortly receive swipe access to the 3d printer room.\n\n\n2.2 Printing the Microscope Body\nTo build your microscope you’ll need to print out the individual parts on the 3d printer. You can use the Original Prusa i3 MK3S+ PLA printers in the HMC Makerspace. To print them, you’ll need the stereolithography (.stl) files. The files are available in the shared folder here under the stl subdirectory. The scad files used to generate the stl files are also included in the folder in the scad subdirectory. If you wish to make any modifications or experiment with the files, you can download OpenSCAD and view the files.\nAfter downloading the files, you’ll need to prepare the prints on the printers in the Makerspace. Either the “0.3 mm DRAFT” or “0.15 mm SPEED” print settings should print at sufficient resolution and a reasonable amount of time.\nTo print the files, you will use the PrusaSlicer software. This software takes the .stl files and slices them into layers which are then translated into directions telling the printer where to move to lay down the individual layers for your object. These instructions for the Prusa printers are written in a file format called .gcode files, which are commonly used in applications for Computer Numerical Control (CNC) machines that are needed by the printers. After slicing and creating the .gcode files, they must be transferred to an SD card and inserted into the SD card slot on the printers in order to print the files.\nThe easiest way to print is to use the computer in the Makerspace 3D print room directly to slice and export the files for the printers since the profiles are already configured properly. You can download a copy of the .gcode file for all the required parts here. If you open this in PrusaSlicer you will see all the parts arranged on the build plate. I’ve printed this on the printer with success, but it’s a little risky because you are doing everything in one shot so if the print fails, you may have multiple failed parts.\nIf you wish, you may also print the parts individually by manually adding them to the build plate using the menu option “File &gt; Import &gt; Import STL/OBJ/AMF/3MF…” option. This will allow you to individually add the parts.\n\n\n\n\n\n\nFigure 2: A screenshot of the Import menu in PrusaSlicer.\n\n\n\nThen you’ll need to move and rotate them to place them on the build plate. Note that you’ll want to orient the parts so that you minimize the number of overhangs since the 3D printer build layer upon layer from bottom to top. Sometimes these are unavoidable though (like on the structure.stl part) and this is ok most of the time even though the print quality for those overhangs is a bit less polished.\nYou can log on to the computer on the left side of the desk to the right of the printers along the wall. Then either download the files from the web to the computer or transfer them using a USB thumb drive. Open them in PrusaSlicer and add them to the build plate. You can print all of the parts together in a single print using the\n\n\n2.3 Assembling the Microscope\nAfter all the parts have been printed, you need to assemble the microscope. There are two main sub-assemblies: the bottom structure which holds the LED array and Pi and the top assembly which holds the spring-loaded camera and lens holder which slides up and down to focus.\nSee the instructions here starting on line 115 describing how to assemble parts.\n\n2.3.1 Installing the Camera Assembly\nThe camera module is fragile and sensitive to static discharge, so try to touch a large grounded metal object (e.g., metal table, faucet, etc.) before beginning this section.\nTo achieve a ~1.5x magnification with the lens currently on the camera you need to remove the camera lens and displace it from the sensor by a few mm. To remove the lens, carefully use the circular white plastic tool that came with the camera to unscrew the lens. Then take the lens all the way off. If your camera module is not already secured to the PCB, remove the small plastic sheet covering the adhesive tape on the back of the module and firmly press it onto the PCB.\nAfter removing the lens, press it into the camera mount with the small aperture facing toward the bottom (in other words, oriented in the same way with respect to the camera sensor as it originally was, just now at a larger offset). Then, attach the camera PCB to the camera mount using the small M1.4 screws in your kit. Make sure the camera cable port is facing out from the trapezoidal side of the mount which connects to the holder.\n\n\n2.3.2 Installing the Camera Mount\nNext slot the camera mount into the mount holder. You may need to slightly stretch the springs if you are having trouble getting them mounted. It is easiest to first connect them to the base of the camera mount using the bar on the bottom and then pushing the entire camera mount unit up in order to get the top side of the springs to attach to the holder near the focus adjustment screw.\nAfter you finish this step, the entire camera mount is finished. Attach it to the flat top piece using two M6 screws. You may need bolts as well, but normally screwing it into the plastic holes is sufficient.\n\n\n\n2.4 Soldering the DotStar LED Array\nThe LED array you have in your kit is the 8x8 DotStar Array from Adafruit. This is a very bright array of RGB LEDs. The array comes with a capacitor which can be used as a decoupling capacitor to smooth out any fluctuations in the power supply voltage. You also have a 4-pin, right-angle male header strip. Before soldering the headers on, use two pairs of needle-nose pliers to bend the header into an S shape. Then solder the short half of the header onto the circular pads on the bottom of the LED array PCB. You will connect to these headers with your hookup wire to connect the LED array to your Raspberry Pi.\n\n\n\n\n\n\nFigure 3: A photo of the bent right-angle header.\n\n\n\n\n\n2.5 Soldering the Pi Header\nYour Raspberry Pi comes with a spot on the board to connect a 2x20 header. You have a 2x20 right-angle female header which you will need to solder to this connection.\n\n\n2.6 Connecting the LED Matrix\nAfter soldering it, you can connect the Raspberry Pi to your LED matrix using the jumper wires in your kit. Before making any connections, disconnect your Pi from USB power.\nSee the instructions here for the pinout, connection diagram, and instructions on using the library. The DotStar array is controlled using the hardware SPI interface on the Pi. Power the LED array by connecting the 5 V and GND pins to the 5 V and GND pins on your Pi. Then connect the SDI pin on the array to the MOSI pin on the Pi (pin 19) and the SCL pin to the array to the SCLK pin on the Pi (pin 23). See the diagram here as a reference. Make sure to double check your connections (best to have a friend check!) to make sure you have everything connected properly before powering it up.\nIn addition to connecting the array to the correct pins, you’ll need to enable the SPI interface in the Pi configuration menu. Run sudo raspi-config and enable SPI under the “Interface Options” sub-menu.\n\n\n2.7 Mounting the LED Array\nBecause the LED array is 8x8, there is no natural center LED. To fix this, the LED array mount is shifted by half an LED pitch up and to the right. This means that the array is now centered at pixel index (3,3) which is the pixel in the 4th row and 4th column counting from the bottom left since the indexing starts at 0. Pixel (0,0) should be located in the bottom left corner when viewing the microscope from the side away from the translation stage. See photo below where LED (0,0) is illuminated. So, to use only a 7x7 subset of the matrix, we should only write to the first 7 rows and columns row_idx = [0,6] and col_idx = [0,6].\n\n\n\n\n\n\nFigure 4: A photo showing the correct alignment of the LED array."
  },
  {
    "objectID": "labs/lab3/index.html#blinky-blinky",
    "href": "labs/lab3/index.html#blinky-blinky",
    "title": "3D-printed Microscope Build",
    "section": "3 Blinky Blinky",
    "text": "3 Blinky Blinky\nAfter setting up your microscope, the first thing we will do is test out the LED array. Activate the virtual environment you set up in Lab 1 (by running source ./.env/bin/activate in your shared directory). Then, we need to install the libraries for the DotStar array (documentation found here). To install it, run sudo pip3 install adafruit-circuitpython-dotstar in your virtual environment. After installing, open up a Python prompt and run the following lines of code.\nimport board\nimport adafruit_dotstar as dotstar\nN_dots = 8*8\ndots = dotstar.DotStar(board.SCK, board.MOSI, N_dots, brightness=0.05)\nAfter importing libraries and creating the DotStar object, try lighting up some single LEDs using a line following the syntax below.\nled_idx = 10\ndots[led_idx] = (255, 0, 0)\nSee if you can figure out the indexing of your array to turn on the LEDs near the center of the array. You can turn off the LED either by writing it to zero (0,0,0) or using the built in functions to clear the array.\nCreate scripts in Python to do each of the following. In this lab we’ll only use the red channel, so you only need to support turning on the red LEDs in the matrix. - Turn on all the LEDs at the same time - Turn on a single LED at a specified x,y location"
  },
  {
    "objectID": "labs/lab3/index.html#microscope-image-capture",
    "href": "labs/lab3/index.html#microscope-image-capture",
    "title": "3D-printed Microscope Build",
    "section": "4 Microscope Image Capture",
    "text": "4 Microscope Image Capture\nAfter figuring out how to control your LED array, the next thing we will do is to take an image and analyze it. For all of these images we will do our analysis using red illumination and only take the data from the red channel of the raw Bayer data.\n\n4.1 Resolution Calculation\nTo measure the resolution of our microscope we will us a standard optical target called a US Air Force (USAF) target. You may read more about the targets here. This target has sets of bars at decreasingly small pitches or separations. These can be used to determine the finest feature size that can be resolved by the optical system. We will talk much more about resolution in future weeks, but for now, you just need to know that the resolution of an optical system is determined by two quantities: the wavelength and the numerical aperture (or equivalently angular acceptance range) of the optical system.\nTo determine the overall resolution of the system we need to take an image and then look at the set of bars with the smallest resolution that we are able to resolve. There are several technical criteria to specific the resolution (e.g., Abbé, Sparrow, Rayleigh), each with their own respective equations, but for the context of this lab we will use Abbé’s definition.\n\\[\nd=\\frac{\\lambda}{(2 n \\sin⁡(\\theta)}=\\frac{\\lambda}{NA_\\text{illumination} + NA_\\text{collection}}\n\\]\nSolving this equation for \\(d\\) yields the minimum distance that can be resolved by our optical system.\nFor our optical system, using the definitions above, answer the following questions:\n\nWhat is the NA of the microscope (recall from Lab 2 or calculate the NA from the f-number and aperture diameter)?\nWhat is the maximum value of the NA in air?\nWhat is the minimum resolvable distance \\(d\\) using red illumination (~630 nm) and the maximum NA in air calculated above?\nWhat is the minimum resolvable distance \\(d\\) using red illumination (~630 nm) and this microscope?\n\n\n\n4.2 Resolution Analysis\nIn this section you will take an image of the USAF target using your microscope and analyze it. Set up your microscope and turn on all the red LEDs. Then, using VNC, use the command line preview tool libcamera to preview the camera by running “libcamera-still -t 0”. This takes an image with an infinite timeout. To exit the window, use ctrl + c. Adjust the focal screw to bring the image into the sharpest focus you can and then exit the preview.\n\n\n\n\n\n\nFigure 5: R1DS1P - Positive 1951 USAF Test Target, Ø1” from Thorlabs (link)\n\n\n\nTake a raw Bayer image of the sample and save it to a .npy file. To capture consistent images, make sure to\n\nTurn off the automatic exposure setting by setting the exposure_mode property of the PiCamera object to “off”\nSet the shutter_speed property to a value which maximizes the dynamic range of the image you capture. In other words, choose a value for the exposure time such that the maximum value of your image spans nearly the whole range of values available for each pixel (you can calculate this value using the bit depth. For example, for an 8-bit depth your pixels can range from 0 to \\(2^8-1=255\\). You should shoot to have the max value of your image reach to roughly 90% of the maximum value without exceeding the value and clipping).\n\nAfter taking the image, transfer the file to your host machine, load it into Python, and plot the red channel. You’ll notice that the raw red channel is sparse (i.e., 75% of the pixels are black). This is because of the Bayer array which splits the channels into red, green, and blue pixels.\nBefore performing your analysis, write Python code to process the raw Bayer data from the red channel and remove the zero entries as shown in the figure below.\n\n\n\n\n\n\nFigure 6: Bayer color filter array single channel selection\n\n\n\nUsing the processed image of the red channel, answer the questions below:\n\nWhat is the effective pixel pitch of the red channel images on the camera sensor plane (i.e., what is the distance between adjacent red pixels?)?\nWhat is the effective pixel pitch of the red channel images on the object plane?\nWhat is the sampling frequency of your image sensor at the sample plane in cycles per millimeter (i.e., the inverse of the pixel pitch at the object plane).\nGiven the minimum detectable feature size \\(d\\) for this microscope from the Abbé resolution equation earlier, what spatial frequency does this correspond to?\nDoes your camera sensor satisfy the Shannon-Nyquist sampling theorem requirement that the sampling frequency must be twice the maximum spatial frequency in the sample field?\nWhat is the minimum resolvable feature in the image? To determine this, create a zoomed-in image of a region of interest of the USAF target where you can start to see the bars blur together. The size of the smallest bar separation that you can measure is a good estimate of your system resolution.\nSave a copy of your zoomed-in ROI image. Make sure to follow good presentation practices and include a scale bar, a figure title, axis labels, and a colorbar.\nCreate a 1D plot of the intensity of perpendicular to three bar target: one that is clearly resolved, one that is just barely resolved, and one that is unresolved. See Figure 3 from the paper for an example of the types of plots that we are looking for here."
  }
]